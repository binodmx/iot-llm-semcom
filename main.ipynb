{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load Dataset",
   "id": "e2fc140285f36a9d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "\n",
    "# =============================================================================\n",
    "# cic-iot\n",
    "# =============================================================================\n",
    "# dataset_name = 'cic-iot'\n",
    "# dataset_path = \"/data/cic-iot.csv\"\n",
    "# df = pd.read_csv(os.getcwd() + dataset_path, low_memory=False)\n",
    "# df['label'] = df['label'].apply(lambda x: \"AttackTraffic\" if x != \"BenignTraffic\" else x)\n",
    "#\n",
    "# plt.figure(figsize=(15, 6))\n",
    "# plt.xticks(rotation=90)\n",
    "# s = sns.countplot(data=df, x='label', hue='label')\n",
    "# for p in s.patches:\n",
    "#     s.annotate(format(p.get_height(), '.0f'),\n",
    "#                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "#                ha = 'center', va = 'center',\n",
    "#                xytext = (0, 9),\n",
    "#                textcoords = 'offset points')\n",
    "# plt.title('Record Count', fontsize=20)\n",
    "\n",
    "# =============================================================================\n",
    "# wustl-iiot\n",
    "# =============================================================================\n",
    "# dataset_name = 'wustl-iiot'\n",
    "# dataset_path = \"/data/wustl-iiot.csv\"\n",
    "# df = pd.read_csv(os.getcwd() + dataset_path, low_memory=False)\n",
    "#\n",
    "# # Encode categorical columns\n",
    "# label_encoder = LabelEncoder()\n",
    "# categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "# for column in categorical_columns:\n",
    "#     df[column] = label_encoder.fit_transform(df[column])\n",
    "#\n",
    "# plt.figure(figsize=(15, 6))\n",
    "# plt.xticks(rotation=90)\n",
    "# s = sns.countplot(data=df, x='Target', hue='Traffic')\n",
    "# for p in s.patches:\n",
    "#     s.annotate(format(p.get_height(), '.0f'),\n",
    "#                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "#                ha = 'center', va = 'center',\n",
    "#                xytext = (0, 9),\n",
    "#                textcoords = 'offset points')\n",
    "# plt.title('Record Count', fontsize=20)\n",
    "\n",
    "# =============================================================================\n",
    "# ton-iot\n",
    "# =============================================================================\n",
    "# dataset_name = 'ton-iot'\n",
    "# dataset_path = \"/data/ton-iot.csv\"\n",
    "# df = pd.read_csv(os.getcwd() + dataset_path, low_memory=False)\n",
    "#\n",
    "# # Encode categorical columns\n",
    "# label_encoder = LabelEncoder()\n",
    "# categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "# for column in categorical_columns:\n",
    "#     df[column] = label_encoder.fit_transform(df[column])\n",
    "#\n",
    "# plt.figure(figsize=(15, 6))\n",
    "# plt.xticks(rotation=90)\n",
    "# s = sns.countplot(data=df, x='label', hue='label')\n",
    "# for p in s.patches:\n",
    "#     s.annotate(format(p.get_height(), '.0f'),\n",
    "#                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "#                ha = 'center', va = 'center',\n",
    "#                xytext = (0, 9),\n",
    "#                textcoords = 'offset points')\n",
    "# plt.title('Record Count', fontsize=20)\n",
    "\n",
    "# =============================================================================\n",
    "# bot-iot\n",
    "# =============================================================================\n",
    "# dataset_name = 'bot-iot'\n",
    "# df_train = pd.read_csv(os.getcwd() + \"/data/bot-iot-train.csv\", low_memory=False)\n",
    "# df_test = pd.read_csv(os.getcwd() + \"/data/bot-iot-test.csv\", low_memory=False)\n",
    "#\n",
    "# # Encode categorical columns\n",
    "# label_encoder = LabelEncoder()\n",
    "# categorical_columns = df_train.select_dtypes(include=['object']).columns\n",
    "# for column in categorical_columns:\n",
    "#     df_train[column] = label_encoder.fit_transform(df_train[column].astype(str))\n",
    "#     df_test[column] = label_encoder.fit_transform(df_test[column].astype(str))\n",
    "#\n",
    "# plt.figure(figsize=(15, 6))\n",
    "# plt.xticks(rotation=90)\n",
    "# s = sns.countplot(data=df_train, x='attack', hue='category')\n",
    "# for p in s.patches:\n",
    "#     s.annotate(format(p.get_height(), '.0f'),\n",
    "#                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "#                ha = 'center', va = 'center',\n",
    "#                xytext = (0, 9),\n",
    "#                textcoords = 'offset points')\n",
    "# plt.title('Record Count', fontsize=20)\n",
    "\n",
    "# =============================================================================\n",
    "# unsw-nb15\n",
    "# =============================================================================\n",
    "# dataset_name = 'unsw-nb15'\n",
    "# df_train = pd.read_csv(os.getcwd() + \"/data/unsw-nb15-train.csv\", low_memory=False)\n",
    "# df_test = pd.read_csv(os.getcwd() + \"/data/unsw-nb15-test.csv\", low_memory=False)\n",
    "#\n",
    "# # Encode categorical columns\n",
    "# label_encoder = LabelEncoder()\n",
    "# categorical_columns = df_train.select_dtypes(include=['object']).columns\n",
    "# for column in categorical_columns:\n",
    "#     df_train[column] = label_encoder.fit_transform(df_train[column].astype(str))\n",
    "#     df_test[column] = label_encoder.fit_transform(df_test[column].astype(str))\n",
    "#\n",
    "# plt.figure(figsize=(15, 6))\n",
    "# plt.xticks(rotation=90)\n",
    "# s = sns.countplot(data=df_train, x='label', hue='attack_cat')\n",
    "# for p in s.patches:\n",
    "#     s.annotate(format(p.get_height(), '.0f'),\n",
    "#                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "#                ha = 'center', va = 'center',\n",
    "#                xytext = (0, 9),\n",
    "#                textcoords = 'offset points')\n",
    "# plt.title('Record Count', fontsize=20)"
   ],
   "id": "630d013dc1fa8fe2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# =============================================================================\n",
    "# cic-iot\n",
    "# =============================================================================\n",
    "# # Separate majority and minority classes\n",
    "# df_majority = df[df.label == df.label.value_counts().idxmax()]\n",
    "# df_minority = df[df.label != df.label.value_counts().idxmax()]\n",
    "#\n",
    "# # Upsample minority class\n",
    "# df_minority_upsampled = resample(df_minority,\n",
    "#                                  replace=True,                  # sample with replacement\n",
    "#                                  n_samples=len(df_majority),    # match majority class\n",
    "#                                  random_state=42)\n",
    "#\n",
    "# # Combine majority class with upsampled minority class\n",
    "# df_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
    "#\n",
    "# # Shuffle\n",
    "# df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "# df = df_balanced\n",
    "#\n",
    "# plt.figure(figsize=(15, 6))\n",
    "# plt.xticks(rotation=90)\n",
    "# s = sns.countplot(data=df, x='label', hue='label')\n",
    "# for p in s.patches:\n",
    "#     s.annotate(format(p.get_height(), '.0f'),\n",
    "#                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "#                ha = 'center', va = 'center',\n",
    "#                xytext = (0, 9),\n",
    "#                textcoords = 'offset points')\n",
    "# plt.title('Record Count', fontsize=20)\n",
    "\n",
    "# =============================================================================\n",
    "# wustl-iiot\n",
    "# =============================================================================\n",
    "# # Separate majority and minority classes\n",
    "# df_majority = df[df.Target == df.Target.value_counts().idxmax()]\n",
    "# df_minority = df[df.Target != df.Target.value_counts().idxmax()]\n",
    "#\n",
    "# # Upsample minority class\n",
    "# df_minority_upsampled = resample(df_minority,\n",
    "#                                  replace=True,                  # sample with replacement\n",
    "#                                  n_samples=len(df_majority),    # match majority class\n",
    "#                                  random_state=42)\n",
    "#\n",
    "# # Combine majority class with upsampled minority class\n",
    "# df_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
    "#\n",
    "# # Shuffle\n",
    "# df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "# df = df_balanced\n",
    "#\n",
    "# plt.figure(figsize=(15, 6))\n",
    "# plt.xticks(rotation=90)\n",
    "# s = sns.countplot(data=df, x='Target', hue='Traffic')\n",
    "# for p in s.patches:\n",
    "#     s.annotate(format(p.get_height(), '.0f'),\n",
    "#                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "#                ha = 'center', va = 'center',\n",
    "#                xytext = (0, 9),\n",
    "#                textcoords = 'offset points')\n",
    "# plt.title('Record Count', fontsize=20)\n",
    "\n",
    "# =============================================================================\n",
    "# ton-iot\n",
    "# =============================================================================\n",
    "# # Separate majority and minority classes\n",
    "# df_majority = df[df.label == df.label.value_counts().idxmax()]\n",
    "# df_minority = df[df.label != df.label.value_counts().idxmax()]\n",
    "#\n",
    "# # Upsample minority class\n",
    "# df_minority_upsampled = resample(df_minority,\n",
    "#                                  replace=True,                  # sample with replacement\n",
    "#                                  n_samples=len(df_majority),    # match majority class\n",
    "#                                  random_state=42)\n",
    "#\n",
    "# # Combine majority class with upsampled minority class\n",
    "# df_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
    "#\n",
    "# # Shuffle\n",
    "# df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "# df = df_balanced\n",
    "#\n",
    "# plt.figure(figsize=(15, 6))\n",
    "# plt.xticks(rotation=90)\n",
    "# s = sns.countplot(data=df, x='label', hue='label')\n",
    "# for p in s.patches:\n",
    "#     s.annotate(format(p.get_height(), '.0f'),\n",
    "#                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "#                ha = 'center', va = 'center',\n",
    "#                xytext = (0, 9),\n",
    "#                textcoords = 'offset points')\n",
    "# plt.title('Record Count', fontsize=20)\n",
    "\n",
    "# =============================================================================\n",
    "# bot-iot\n",
    "# =============================================================================\n",
    "# # Separate majority and minority classes\n",
    "# df_train_majority = df_train[df_train.attack == df_train.attack.value_counts().idxmax()]\n",
    "# df_train_minority = df_train[df_train.attack != df_train.attack.value_counts().idxmax()]\n",
    "# df_test_majority = df_test[df_test.attack == df_test.attack.value_counts().idxmax()]\n",
    "# df_test_minority = df_test[df_test.attack != df_test.attack.value_counts().idxmax()]\n",
    "#\n",
    "# # Upsample minority class\n",
    "# df_train_minority_upsampled = resample(df_train_minority,\n",
    "#                                  replace=True,                  # sample with replacement\n",
    "#                                  n_samples=len(df_train_majority),    # match majority class\n",
    "#                                  random_state=42)\n",
    "# df_test_minority_upsampled = resample(df_test_minority,\n",
    "#                                     replace=True,                  # sample with replacement\n",
    "#                                     n_samples=len(df_test_majority),    # match majority class\n",
    "#                                     random_state=42)\n",
    "#\n",
    "# # Combine majority class with upsampled minority class\n",
    "# df_train_balanced = pd.concat([df_train_majority, df_train_minority_upsampled])\n",
    "# df_test_balanced = pd.concat([df_test_majority, df_test_minority_upsampled])\n",
    "#\n",
    "# # Shuffle\n",
    "# df_train_balanced = df_train_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "# df_test_balanced = df_test_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "# df_train = df_train_balanced\n",
    "# df_test = df_test_balanced\n",
    "#\n",
    "# plt.figure(figsize=(15, 6))\n",
    "# plt.xticks(rotation=90)\n",
    "# s = sns.countplot(data=df_train, x='attack', hue='category')\n",
    "# for p in s.patches:\n",
    "#     s.annotate(format(p.get_height(), '.0f'),\n",
    "#                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "#                ha = 'center', va = 'center',\n",
    "#                xytext = (0, 9),\n",
    "#                textcoords = 'offset points')\n",
    "# plt.title('Record Count', fontsize=20)\n",
    "\n",
    "# =============================================================================\n",
    "# unsw-nb15\n",
    "# =============================================================================\n",
    "# # Separate majority and minority classes\n",
    "# df_train_majority = df_train[df_train.label == df_train.label.value_counts().idxmax()]\n",
    "# df_train_minority = df_train[df_train.label != df_train.label.value_counts().idxmax()]\n",
    "# df_test_majority = df_test[df_test.label == df_test.label.value_counts().idxmax()]\n",
    "# df_test_minority = df_test[df_test.label != df_test.label.value_counts().idxmax()]\n",
    "#\n",
    "# # Upsample minority class\n",
    "# df_train_minority_upsampled = resample(df_train_minority,\n",
    "#                                  replace=True,                  # sample with replacement\n",
    "#                                  n_samples=len(df_train_majority),    # match majority class\n",
    "#                                  random_state=42)\n",
    "# df_test_minority_upsampled = resample(df_test_minority,\n",
    "#                                     replace=True,                  # sample with replacement\n",
    "#                                     n_samples=len(df_test_majority),    # match majority class\n",
    "#                                     random_state=42)\n",
    "#\n",
    "# # Combine majority class with upsampled minority class\n",
    "# df_train_balanced = pd.concat([df_train_majority, df_train_minority_upsampled])\n",
    "# df_test_balanced = pd.concat([df_test_majority, df_test_minority_upsampled])\n",
    "#\n",
    "# # Shuffle\n",
    "# df_train_balanced = df_train_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "# df_test_balanced = df_test_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "# df_train = df_train_balanced\n",
    "# df_test = df_test_balanced\n",
    "#\n",
    "# plt.figure(figsize=(15, 6))\n",
    "# plt.xticks(rotation=90)\n",
    "# s = sns.countplot(data=df_train, x='label', hue='attack_cat')\n",
    "# for p in s.patches:\n",
    "#     s.annotate(format(p.get_height(), '.0f'),\n",
    "#                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "#                ha = 'center', va = 'center',\n",
    "#                xytext = (0, 9),\n",
    "#                textcoords = 'offset points')\n",
    "# plt.title('Record Count', fontsize=20)"
   ],
   "id": "3a178ec2a7d81709",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T10:20:28.158360Z",
     "start_time": "2025-05-12T10:20:28.012311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# =============================================================================\n",
    "# cic-iot\n",
    "# =============================================================================\n",
    "# X = df.drop(columns='label')\n",
    "# y = df['label']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "#     test_size=0.2,               # 80% train, 20% test (can adjust)\n",
    "#     stratify=y,                  # This preserves the class ratio\n",
    "#     random_state=42              # For reproducibility\n",
    "# )\n",
    "# feature_names = X_train.columns.tolist()\n",
    "\n",
    "# =============================================================================\n",
    "# wustl-iiot\n",
    "# =============================================================================\n",
    "# X = df.drop(columns=['Target', 'Traffic'])\n",
    "# y = df['Target']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "#     test_size=0.2,               # 80% train, 20% test (can adjust)\n",
    "#     stratify=y,                  # This preserves the class ratio\n",
    "#     random_state=42              # For reproducibility\n",
    "# )\n",
    "# feature_names = X_train.columns.tolist()\n",
    "\n",
    "# =============================================================================\n",
    "# ton-iot\n",
    "# =============================================================================\n",
    "# X = df.drop(columns=['label', 'type'])\n",
    "# y = df['label']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "#     test_size=0.2,               # 80% train, 20% test (can adjust)\n",
    "#     stratify=y,                  # This preserves the class ratio\n",
    "#     random_state=42              # For reproducibility\n",
    "# )\n",
    "# feature_names = X_train.columns.tolist()\n",
    "\n",
    "# =============================================================================\n",
    "# bot-iot\n",
    "# =============================================================================\n",
    "# X_train = df_train.drop(columns=['attack', 'category', 'subcategory', 'pkSeqID'])\n",
    "# y_train = df_train['attack']\n",
    "# X_test = df_test.drop(columns=['attack', 'category', 'subcategory', 'pkSeqID'])\n",
    "# y_test = df_test['attack']\n",
    "# feature_names = X_train.columns.tolist()\n",
    "\n",
    "# =============================================================================\n",
    "# unsw-nb15\n",
    "# =============================================================================\n",
    "# X_train = df_train.drop(columns=['id', 'attack_cat', 'label'])\n",
    "# y_train = df_train['label']\n",
    "# X_test = df_test.drop(columns=['id', 'attack_cat', 'label'])\n",
    "# y_test = df_test['label']\n",
    "# feature_names = X_train.columns.tolist()"
   ],
   "id": "626cb822f56a49fb",
   "outputs": [],
   "execution_count": 348
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Set Configurations",
   "id": "b268215a9d2230a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T10:20:32.975004Z",
     "start_time": "2025-05-12T10:20:32.971387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_models = 10\n",
    "llm_name = \"gpt-4.1-mini\""
   ],
   "id": "a8e2b56476102670",
   "outputs": [],
   "execution_count": 349
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train Decision Tree Classifiers",
   "id": "832722193851eec5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.tree import export_text\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "models_4 = []\n",
    "tree_texts_4 = []\n",
    "\n",
    "px_size = len(X_train) // n_models\n",
    "max_depth = 4\n",
    "max_leaf_nodes = 8\n",
    "max_features = 7\n",
    "\n",
    "for i in tqdm(range(n_models), ncols=100, desc=f\"Generating tree texts\"):\n",
    "    model = DecisionTreeClassifier(\n",
    "        max_depth=max_depth,\n",
    "        max_leaf_nodes=max_leaf_nodes,\n",
    "        max_features=max_features,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train[i*px_size:i*px_size+px_size], y_train[i*px_size:min(i*px_size+px_size,len(X_train))])\n",
    "    models_4.append(model)\n",
    "    tree_text = export_text(model, feature_names=feature_names, max_depth=max_depth)\n",
    "    tree_texts_4.append(tree_text)"
   ],
   "id": "2e636e5a797e2d22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.tree import export_text\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "models_3 = []\n",
    "tree_texts_3 = []\n",
    "\n",
    "px_size = len(X_train) // n_models\n",
    "max_depth = 3\n",
    "max_leaf_nodes = 4\n",
    "max_features = 3\n",
    "\n",
    "for i in tqdm(range(n_models), ncols=100, desc=f\"Generating tree texts\"):\n",
    "    model = DecisionTreeClassifier(\n",
    "        max_depth=max_depth,\n",
    "        max_leaf_nodes=max_leaf_nodes,\n",
    "        max_features=max_features,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train[i*px_size:i*px_size+px_size], y_train[i*px_size:min(i*px_size+px_size,len(X_train))])\n",
    "    models_3.append(model)\n",
    "    tree_text = export_text(model, feature_names=feature_names, max_depth=max_depth)\n",
    "    tree_texts_3.append(tree_text)"
   ],
   "id": "8ed95b250b7e6c7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T10:21:13.939913Z",
     "start_time": "2025-05-12T10:21:13.932849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save models as pickle files\n",
    "import pickle\n",
    "\n",
    "with open(os.getcwd() + f'/models/{dataset_name}_{n_models}_dts_4.pkl', 'wb') as f:\n",
    "    pickle.dump(models_4, f)\n",
    "\n",
    "with open(os.getcwd() + f'/models/{dataset_name}_{n_models}_dts_3.pkl', 'wb') as f:\n",
    "    pickle.dump(models_3, f)\n",
    "\n",
    "with open(os.getcwd() + f'/tree-texts/{dataset_name}_{n_models}_tts_4.pkl', 'wb') as f:\n",
    "    pickle.dump(tree_texts_4, f)\n",
    "\n",
    "with open(os.getcwd() + f'/tree-texts/{dataset_name}_{n_models}_tts_3.pkl', 'wb') as f:\n",
    "    pickle.dump(tree_texts_3, f)"
   ],
   "id": "1804b52de1148c0a",
   "outputs": [],
   "execution_count": 352
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T03:36:29.521718Z",
     "start_time": "2025-05-10T03:36:29.510568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load models from pickle files\n",
    "import pickle\n",
    "\n",
    "with open(os.getcwd() + f'/models/{dataset_name}_{n_models}_dts_4.pkl', 'rb') as f:\n",
    "    models_4 = pickle.load(f)\n",
    "\n",
    "with open(os.getcwd() + f'/models/{dataset_name}_{n_models}_dts_3.pkl', 'rb') as f:\n",
    "    models_3 = pickle.load(f)\n",
    "\n",
    "with open(os.getcwd() + f'/tree-texts/{dataset_name}_{n_models}_tts_4.pkl', 'rb') as f:\n",
    "    tree_texts_4 = pickle.load(f)\n",
    "\n",
    "with open(os.getcwd() + f'/tree-texts/{dataset_name}_{n_models}_tts_3.pkl', 'rb') as f:\n",
    "    tree_texts_3 = pickle.load(f)"
   ],
   "id": "a21bf63dd72924e3",
   "outputs": [],
   "execution_count": 263
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Semantic Encoding",
   "id": "f15451812a93f250"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from Codec import Codec\n",
    "from tqdm import tqdm\n",
    "\n",
    "semantic_encoder = Codec(\"semantic_encoder\", model_name=llm_name)\n",
    "\n",
    "encoded_ai_messages = []\n",
    "for i in tqdm(range(n_models), ncols=100, desc=\"Encoding decision trees\"):\n",
    "    system_message = SystemMessage(content=\"\"\"\n",
    "    You are given a decision tree represented as text.\n",
    "    Each line shows a split based on a feature and threshold, with indentation indicating tree depth.\n",
    "    1. First, extract all the paths from the root to the leaves of the tree.\n",
    "    2. Then identify the most important decision rules from these paths composed of feature thresholds that lead to a class label.\n",
    "    3. Finally, ONLY output 3 most important decision rules.\n",
    "    \"\"\")\n",
    "    human_message = HumanMessage(content=f\"\"\"\n",
    "    Analyze the decision tree.\n",
    "    ONLY output 3 most important decision rules.\n",
    "\n",
    "    Here is the decision tree text:\n",
    "    ```\n",
    "    {tree_texts_4[i]}\n",
    "    ```\n",
    "    \"\"\"\n",
    "    )\n",
    "    final_state = semantic_encoder.invoke({\"messages\": [system_message, human_message]})\n",
    "    encoded_ai_message = final_state['messages'][-1]\n",
    "    encoded_ai_messages.append(encoded_ai_message.content)"
   ],
   "id": "62a0078a62d059c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T13:51:40.750645Z",
     "start_time": "2025-05-12T13:51:40.735989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save encoded messages as pickle files\n",
    "import pickle\n",
    "\n",
    "with open(os.getcwd() + f'/encoded-texts/{dataset_name}_{llm_name}_ets_4.pkl', 'wb') as f:\n",
    "    pickle.dump(encoded_ai_messages, f)"
   ],
   "id": "d58b3e70a22510e",
   "outputs": [],
   "execution_count": 354
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T04:03:18.740517Z",
     "start_time": "2025-05-08T04:03:18.738101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load encoded messages from pickle files\n",
    "import pickle\n",
    "\n",
    "with open(os.getcwd() + f'/encoded-texts/{dataset_name}_{llm_name}_ets_4.pkl', 'rb') as f:\n",
    "    encoded_ai_messages = pickle.load(f)"
   ],
   "id": "71838a8e35f32e2c",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Semantic Decoding",
   "id": "557793651be309b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from Codec import Codec\n",
    "from tqdm import tqdm\n",
    "\n",
    "semantic_decoder = Codec(\"semantic_decoder\", model_name=llm_name)\n",
    "\n",
    "decoded_ai_messages = []\n",
    "for i in tqdm(range(n_models), ncols=100, desc=\"Decoding decision trees\"):\n",
    "    system_message = SystemMessage(content=\"\"\"\n",
    "    You are a highly skilled AI model specialized in decision tree modification.\n",
    "    Given a decision tree text, your task is to modify the tree by replacing some decision rules.\n",
    "    1. First, extract all the paths from the root to the leaves of the tree.\n",
    "    2. Identify redundant features that can be replaced using the provided information.\n",
    "    3. Make sure to keep two nodes for same feature (for <= and >) otherwise ignore the feature.\n",
    "    4. ONLY output the modified decision tree text.\n",
    "    \"\"\")\n",
    "    human_message = HumanMessage(content=f\"\"\"\n",
    "    Analyze the give decision tree text and refine using the given information.\n",
    "\n",
    "    Decision tree text:\n",
    "    ```\n",
    "{tree_texts_3[i]}\n",
    "    ```\n",
    "\n",
    "    Information about better decision tree:\n",
    "    ```\n",
    "    {encoded_ai_messages[i]}\n",
    "    ```\n",
    "\n",
    "    ONLY output the modified decision tree text between triple backticks.\n",
    "    \"\"\"\n",
    "    )\n",
    "    final_state = semantic_decoder.invoke({\"messages\": [system_message, human_message]})\n",
    "    decoded_ai_message = final_state['messages'][-1]\n",
    "    decoded_ai_messages.append(decoded_ai_message.content)"
   ],
   "id": "a70a63672a030a8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T14:41:23.285153Z",
     "start_time": "2025-05-12T14:41:23.280260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save decoded messages as pickle files\n",
    "import pickle\n",
    "\n",
    "with open(os.getcwd() + f'/decoded-texts/{dataset_name}_{llm_name}_dts_3.pkl', 'wb') as f:\n",
    "    pickle.dump(decoded_ai_messages, f)"
   ],
   "id": "5068b64dac7cf5c",
   "outputs": [],
   "execution_count": 382
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T03:59:08.987956Z",
     "start_time": "2025-05-10T03:59:08.982793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load decoded messages from pickle files\n",
    "import pickle\n",
    "\n",
    "with open(os.getcwd() + f'/decoded-texts/{dataset_name}_{llm_name}_dts_3.pkl', 'rb') as f:\n",
    "    decoded_ai_messages = pickle.load(f)"
   ],
   "id": "a8dd57f2844cee0d",
   "outputs": [],
   "execution_count": 309
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Validate Decision Trees",
   "id": "20ca432f6763c735"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from Validator import Validator\n",
    "from tqdm import tqdm\n",
    "from langchain_core.tools import tool\n",
    "from ParsedDecisionTreeClassifier import ParsedDecisionTreeClassifier\n",
    "\n",
    "@tool\n",
    "def validate(tree_text):\n",
    "    \"\"\"\n",
    "    Validate the given decision tree text by attempting to execute after parsing it.\n",
    "    If the execution is successful, the tree text is valid.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        clf = ParsedDecisionTreeClassifier(tree_text, feature_names)\n",
    "        clf.validate_tree()\n",
    "        return \"This decision tree text is valid.\"\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return f\"This decision tree text is invalid due to: {e}\"\n",
    "\n",
    "dt_validator = Validator(\"dt_validator\", model_name=llm_name, validator_func=validate)\n",
    "\n",
    "validated_ai_messages = []\n",
    "for i in tqdm(range(n_models), ncols=100, desc=\"Validating decision trees\"):\n",
    "    system_message = SystemMessage(content=f\"\"\"\n",
    "        You are a highly skilled AI model specialized in decision tree error handling.\n",
    "        1. First, given a decision tree text, your task is to identify and fix the errors.\n",
    "            1.1. Check the given tree satisfy the binary tree structure.\n",
    "            1.2. Check each feature evaluation has <= and > thresholds.\n",
    "            1.3. Check whether each condition node has its opposite condition node.\n",
    "            1.4. Check whether feature names are in {feature_names}.\n",
    "        2. Then, use the 'validate' tool to validate the modified decision tree text until the tree text is valid.\n",
    "        3. Finally, output the correct decision tree text between triple backticks.\n",
    "    \"\"\")\n",
    "    human_message = HumanMessage(content=f\"\"\"\n",
    "        Validate the following decision tree text.\n",
    "\n",
    "        Decision tree text:\n",
    "        ```\n",
    "    {decoded_ai_messages[i]}\n",
    "        ```\n",
    "    \"\"\")\n",
    "    final_state = dt_validator.invoke({\"messages\": [system_message, human_message]})\n",
    "    validated_ai_message = final_state['messages'][-1]\n",
    "    validated_ai_messages.append(validated_ai_message.content)"
   ],
   "id": "3ff05a27d210abc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T14:41:44.904638Z",
     "start_time": "2025-05-12T14:41:44.900051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save validated messages as pickle files\n",
    "import pickle\n",
    "\n",
    "with open(os.getcwd() + f'/validated-texts/{dataset_name}_{llm_name}_vts_3.pkl', 'wb') as f:\n",
    "    pickle.dump(validated_ai_messages, f)"
   ],
   "id": "1467c4d855768e11",
   "outputs": [],
   "execution_count": 383
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T10:25:53.591157Z",
     "start_time": "2025-05-09T10:25:53.586727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load validated messages from pickle files\n",
    "import pickle\n",
    "\n",
    "with open(os.getcwd() + f'/validated-texts/{dataset_name}_{llm_name}_vts_3.pkl', 'rb') as f:\n",
    "    validated_ai_messages = pickle.load(f)"
   ],
   "id": "fd863ac59bbe346b",
   "outputs": [],
   "execution_count": 169
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluate",
   "id": "366ed66a521d76bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from ParsedDecisionTreeClassifier import ParsedDecisionTreeClassifier\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "f1_scores_4 = []\n",
    "f1_scores_3 = []\n",
    "f1_scores_3_improved = []\n",
    "f1_scores_3_improved_nov = []\n",
    "\n",
    "c_reports_4 = []\n",
    "c_reports_3 = []\n",
    "c_reports_3_improved = []\n",
    "c_reports_3_improved_nov = []\n",
    "\n",
    "cms_4 = []\n",
    "cms_3 = []\n",
    "cms_3_improved = []\n",
    "cms_3_improved_nov = []\n",
    "\n",
    "for i in tqdm(range(n_models), ncols=100, desc=f\"Evaluating decision trees of depth 4\"):\n",
    "    model = models_4[i]\n",
    "    y_true = y_test\n",
    "    y_pred = model.predict(X_test)\n",
    "    c_report = classification_report(y_true, y_pred, digits=4, output_dict=True)\n",
    "    c_reports_4.append(c_report)\n",
    "    labels = np.unique(y_true)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    cms_4.append(cm)\n",
    "    f1_scores_4.append(c_report['macro avg']['f1-score'])\n",
    "\n",
    "with open(os.getcwd() + f'/classification-reports/{dataset_name}_{llm_name}_cr_4.pkl', 'wb') as f:\n",
    "    pickle.dump(c_reports_4, f)\n",
    "\n",
    "with open(os.getcwd() + f'/confusion-matrices/{dataset_name}_{llm_name}_cm_4.pkl', 'wb') as f:\n",
    "    pickle.dump(cms_4, f)\n",
    "\n",
    "for i in tqdm(range(n_models), ncols=100, desc=f\"Evaluating decision trees of depth 3\"):\n",
    "    model = models_3[i]\n",
    "    y_true = y_test\n",
    "    y_pred = model.predict(X_test)\n",
    "    c_report = classification_report(y_true, y_pred, digits=4, output_dict=True)\n",
    "    c_reports_3.append(c_report)\n",
    "    labels = np.unique(y_true)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    cms_3.append(cm)\n",
    "    f1_scores_3.append(c_report['macro avg']['f1-score'])\n",
    "\n",
    "with open(os.getcwd() + f'/classification-reports/{dataset_name}_{llm_name}_cr_3.pkl', 'wb') as f:\n",
    "    pickle.dump(c_reports_3, f)\n",
    "\n",
    "with open(os.getcwd() + f'/confusion-matrices/{dataset_name}_{llm_name}_cm_3.pkl', 'wb') as f:\n",
    "    pickle.dump(cms_3, f)\n",
    "\n",
    "for i in tqdm(range(n_models), ncols=100, desc=\"Evaluating improved decision trees\"):\n",
    "    try:\n",
    "        clf = ParsedDecisionTreeClassifier(\n",
    "            validated_ai_messages[i].split('```')[1].split('```')[0],\n",
    "            feature_names)\n",
    "        y_true = y_test.astype(str)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        c_report = classification_report(y_true, y_pred, digits=4, output_dict=True)\n",
    "        labels = np.unique(y_true)\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "        f1_score = c_report['macro avg']['f1-score']\n",
    "    except Exception as e:\n",
    "        f1_score = 0.0\n",
    "        c_report = None\n",
    "        cm = None\n",
    "        print(e)\n",
    "    c_reports_3_improved.append(c_report)\n",
    "    cms_3_improved.append(cm)\n",
    "    f1_scores_3_improved.append(f1_score)\n",
    "\n",
    "with open(os.getcwd() + f'/classification-reports/{dataset_name}_{llm_name}_cr_3_improved.pkl', 'wb') as f:\n",
    "    pickle.dump(c_reports_3_improved, f)\n",
    "\n",
    "with open(os.getcwd() + f'/confusion-matrices/{dataset_name}_{llm_name}_cm_3_improved.pkl', 'wb') as f:\n",
    "    pickle.dump(cms_3_improved, f)\n",
    "\n",
    "for i in tqdm(range(n_models), ncols=100, desc=\"Evaluating improved decision trees\"):\n",
    "    try:\n",
    "        clf = ParsedDecisionTreeClassifier(\n",
    "            decoded_ai_messages[i].split('```')[1].split('```')[0],\n",
    "            feature_names)\n",
    "        y_true = y_test.astype(str)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        c_report = classification_report(y_true, y_pred, digits=4, output_dict=True)\n",
    "        labels = np.unique(y_true)\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "        f1_score = c_report['macro avg']['f1-score']\n",
    "    except Exception as e:\n",
    "        f1_score = 0.0\n",
    "        c_report = None\n",
    "        cm = None\n",
    "        print(e)\n",
    "    c_reports_3_improved_nov.append(c_report)\n",
    "    cms_3_improved_nov.append(cm)\n",
    "    f1_scores_3_improved_nov.append(f1_score)\n",
    "\n",
    "with open(os.getcwd() + f'/classification-reports/{dataset_name}_{llm_name}_cr_3_improved_nov.pkl', 'wb') as f:\n",
    "    pickle.dump(c_reports_3_improved_nov, f)\n",
    "\n",
    "with open(os.getcwd() + f'/confusion-matrices/{dataset_name}_{llm_name}_cm_3_improved_nov.pkl', 'wb') as f:\n",
    "    pickle.dump(cms_3_improved_nov, f)"
   ],
   "id": "62588768275edae6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
